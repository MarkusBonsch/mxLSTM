% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mxLSTM.R, R/mxLSTMsource.R
\docType{package}
\name{mxLSTM}
\alias{mxLSTM}
\alias{mxLSTM-package}
\alias{mxLSTM}
\title{mxLSTM: A library facilitating regression analysis with LSTMs}
\usage{
mxLSTM(x, y, num.epoch, test.x = NULL, test.y = NULL, num.hidden,
  dropoutLstm = num.hidden * 0, zoneoutLstm = num.hidden * 0,
  batchNormLstm = FALSE, batch.size = 128, activation = "relu",
  optimizer = "rmsprop", initializer = mx.init.Xavier(), shuffle = TRUE,
  initialModel = NULL, ...)
}
\arguments{
\item{x}{array containing the features:
\itemize{
  \item{Dimension 1:} one entry for each feature
  \item{Dimension 2:} one entry for each element in the sequence
  \item{Dimension 3:} one entry for each training event
  }
  Use \code{\link{transformLSTMinput}} to transform data.frames into this structure.}

\item{y}{array containing the target labels:
\itemize{
  \item{Dimension 1:} one entry for each output variable
  \item{Dimension 2:} one entry for each element in the sequence
  \item{Dimension 3:} one entry for each training event
  }
  Use \code{\link{transformLSTMinput}} to transform data.frames into this structure.}

\item{num.epoch}{integer number of training epochs over ful ldataset}

\item{test.x}{same as x, but for testing, not for training}

\item{test.y}{same as y but for testing, not for training}

\item{num.hidden}{integer vector of flexible length. For each entry, an LSTM layer is with the corresponding number of
neurons is created.}

\item{dropoutLstm}{numeric vector of same length as num.hidden. Specifies the dropout probability for each LSTM layer.
Dropout is applied according to Cheng et al. "An exploration of dropout with LSTMs". 
Difference: we employ a constant dropout rate; we do per element dropout.}

\item{zoneoutLstm}{numeric vector of same length as num.hidden. Specifies the zoneout probability for each LSTM layer.
Zoneout is implemented according to 
Krueger et al. 2017 "Zoneout: Regularizing RNNs by randomly preserving hidden activations".}

\item{batchNormLstm}{logical. If TRUE, each LSTM layer is batch normalized according to the recommendations in
T. Cooljmans et al. ILRC 2017 "Recurrent batch normalization".}

\item{batch.size}{self explanatory}

\item{activation}{activation function for update layers in the LSTM cells. "relu" or "tanh"}

\item{optimizer}{character specifying the type of optimizer to use.}

\item{initializer}{random initializer for weights}

\item{shuffle}{Boolean. Should the training data be reordered randomly prior to training? 
(reorders full sequences, order within each sequence is unaffected.)}

\item{initialModel}{mxLSTM model. If provided, all weights are initialized based on the given model.}

\item{...}{Additional arguments to optimizer}
}
\value{
object of class mxLSTM: list: a symbol, arg.params, aux.params, a log, and the variable names
}
\description{
Provides functions for doing and evaluating regressions with LSTMs.

Builds an LSTM model
}
\details{
sequence length is inferred from input (dimension 2).
}
\seealso{
\code{\link{fitLSTMmodel}}, \code{\link{predictLSTMmodel}}, \code{\link{getLSTMmodel}},
         \code{\link{plot_trainHistory}}
}
