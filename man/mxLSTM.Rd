% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mxLSTM.R, R/mxLSTMsource.R
\docType{package}
\name{mxLSTM}
\alias{mxLSTM}
\alias{mxLSTM-package}
\alias{mxLSTM}
\title{mxLSTM: A library facilitating regression analysis with LSTMs}
\usage{
mxLSTM(x, y, num.epoch, test.x = NULL, test.y = NULL, num.hidden,
  dropoutLstm = num.hidden * 0, zoneoutLstm = num.hidden * 0,
  batchNormLstm = FALSE, gammaInit = 0.1, batch.size = 128,
  activation = "relu", optimizer = "rmsprop",
  initializer = mx.init.Xavier(), shuffle = TRUE, initialModel = NULL,
  ...)
}
\arguments{
\item{x}{array containing the features:
\itemize{
  \item{Dimension 1:} one entry for each feature
  \item{Dimension 2:} one entry for each element in the sequence
  \item{Dimension 3:} one entry for each training event
  }
  Use \code{\link{transformLSTMinput}} to transform data.frames into this structure.}

\item{y}{array containing the target labels:
\itemize{
  \item{Dimension 1:} one entry for each output variable
  \item{Dimension 2:} one entry for each element in the sequence
  \item{Dimension 3:} one entry for each training event
  }
  Use \code{\link{transformLSTMinput}} to transform data.frames into this structure.}

\item{num.epoch}{integer number of training epochs over full ldataset}

\item{test.x}{same as x, but for testing, not for training}

\item{test.y}{same as y but for testing, not for training}

\item{num.hidden}{integer vector of flexible length. For each entry, an LSTM layer is with the corresponding number of
neurons is created.}

\item{dropoutLstm}{numeric vector of same length as num.hidden. Specifies the dropout probability for each LSTM layer.
Dropout is applied according to Cheng et al. "An exploration of dropout with LSTMs". 
Difference: we employ a constant dropout rate; we do per element dropout.}

\item{zoneoutLstm}{numeric vector of same length as num.hidden. Specifies the zoneout probability for each LSTM layer.
Zoneout is implemented according to 
Krueger et al. 2017 "Zoneout: Regularizing RNNs by randomly preserving hidden activations".}

\item{batchNormLstm}{logical. If TRUE, each LSTM layer is batch normalized according to the recommendations in
T. Cooljmans et al. ILRC 2017 "Recurrent batch normalization".}

\item{gammaInit}{numeric value. Will be used to initialize the gamma matrices of batchNormLayers. 
Cooljmans et al. recommend 0.1 (for use with tanh activation), mxnet default is 1.
My experience: 0.1 works very badly with relu activation.}

\item{batch.size}{self explanatory}

\item{activation}{activation function for update layers in the LSTM cells. "relu" or "tanh"}

\item{optimizer}{character specifying the type of optimizer to use.}

\item{initializer}{random initializer for weights}

\item{shuffle}{Boolean. Should the training data be reordered randomly prior to training? 
(reorders full sequences, order within each sequence is unaffected.)}

\item{initialModel}{mxLSTM model. If provided, all weights are initialized based on the given model.}

\item{...}{Additional arguments to optimizer}
}
\value{
object of class mxLSTM: list: a symbol, arg.params, aux.params, a log, and the variable names
}
\description{
Provides functions for doing and evaluating regressions with LSTMs.

Builds an LSTM model
}
\details{
sequence length is inferred from input (dimension 2).
}
\examples{
\dontrun{
library(mxLSTM)
library(data.table)

## simple data: one numeric output as a function of two numeric inputs.
## including lag values
## with some noise.
dat <- data.table(x = runif(n = 8000, min = 1000, max = 2000),
                  y = runif(n = 8000, min = -10, max = 10))
## create target
dat[, target := 0.5 * x + 0.7 * lag(y, 3) - 0.2 * lag(x, 5)]
dat[, target := target + rnorm(8000, 0, 10)]
## convert to nxLSTM input
dat <- transformLSTMinput(dat = dat, targetColumn = "target", seq.length = 5)

## train model
model <- mxLSTM(x = dat$x, y = dat$y, num.epoch = 10, num.hidden = 64, 
                dropoutLstm = 0, zoneoutLstm = 0, batchNormLstm = FALSE, batch.size = 128)

## plot training history
plot_trainHistory(model)

## get some predictions (on training set)
predTrain <- predictLSTMmodel(model = model, dat = dat$x, fullSequence = FALSE)

## nice plot
plot_goodnessOfFit(predicted = predTrain$y, observed = dat$y[5,])
}
}
\seealso{
\code{\link{fitLSTMmodel}}, \code{\link{predictLSTMmodel}}, \code{\link{getLSTMmodel}},
         \code{\link{plot_trainHistory}}
}
